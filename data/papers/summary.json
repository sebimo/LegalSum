[
    {
        "title": "TLDR: Extreme Summarization of Scientific Documents",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["SCITLDR"],
        "method": ["Multitask", "BART", "Transformer"],
        "baseline": ["PACSUM", "TextRank", "BERTSumExt"]
    },
    {
        "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Salient Sentence Selection", "Policy Gradient", "Combined", "Temporal CNN", "LSTM", "RL", "Encoder-Aligner-Decoder", "Copy", "Reranking"],
        "baseline": ["lead-3", "FF-Ext", "RNN-Ext", "FF-Ext+Abs", "RNN-Ext+Abs"]
    },
    {
        "title": "Combining Different Summarization Techniques for Legal Text",
        "date": "2012",
        "type": "extractive",
        "dataset": ["AustLII"],
        "method": ["Rules", "Feature Engineering"],
        "baseline": []
    },
    {
        "title": "Mind The Facts: Knowledge-Boosted Coherent Abstractive Text Summarization",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Transformer", "Entity Data"],
        "baseline": ["Transformer"]
    },
    {
        "title": "Improving Abstraction in Text Summarization",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Novelty", "Policy Gradient", "Encoder-Decoder", "Temporal Attention", "Intra Attention", "RL", "LSTM"],
        "baseline": ["Pointer", "SumGAN", "RSal"]
    },
    {
        "title": "Text Summarization with Pretrained Encoders",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["CNN", "NYT", "XSum"],
        "method": ["BERT", "Intersentence Encoder", "Encoder-Decoder", "Finetuning"],
        "baseline": ["oracle", "lead-3", "Pointer", "Compress", "Sumo"]
    },
    {
        "title": "NLP Based Latent Semantic Analysis for Legal Text Summarization",
        "date": "2018",
        "type": "extractive",
        "dataset": ["Legal"],
        "method": ["LSA"],
        "baseline": []
    },
    {
        "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond",
        "date": "2016",
        "type": "abstractive",
        "dataset": ["Gigaword", "DUC", "CNN"],
        "method": ["GRU", "Encoder-Decoder", "LVT", "Feature Engineering", "Pointer", "Hierarchical", "Attention", "word2vec"],
        "baseline": []
    },
    {
        "title": "Get To The Point: Summarization with Pointer-Generator Networks",
        "date": "2017",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["seq2seq", "Attention", "Pointer", "Coverage"],
        "baseline": ["Nallapati", "lead-3"]
    },
    {
        "title": "Abstractive and mixed summarization for long-single documents",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["Arxiv"],
        "method": ["RNN", "CNN", "Transformer", "RL", "Attention"],
        "baseline": []
    },
    {
        "title": "At Which Level Should We Extract? An Empirical Study on Extractive Document Summarization",
        "date": "2020",
        "type": "extractive",
        "dataset": ["CNN"],
        "method": ["BERT", "Transformer"],
        "baseline": ["Pointer", "DCA", "FastRewrite", "InconsisLoss", "Bottom-Up", "lead-3", "TextRank", "SummmaRunner", "NN-SE", "Refresh", "BanditSum", "NeuSum", "JECS", "BERTSUM", "SELF-SUPERVISED"]
    },
    {
        "title": "Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",
        "date": "2018",
        "type": "abstractive",
        "dataset": [],
        "method": ["Template", "seq2seq", "IR", "Reranking"],
        "baseline": ["ABS", "RAS-Elman", "Featseq2seq", "Luong-NMT", "OpenNMT", "FTSum"]
    },
    {
        "title": "Deep Communicating Agents for Abstractive Summarization",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN", "NYT"],
        "method": ["Hierarchical", "RL", "LSTM", "Attention", "Pointer", "Coverage"],
        "baseline": ["SummmaRunner", "graph attention", "Pointer", "Fan", "DeepRL", "MLE-Pointer"]
    },
    {
        "title": "Distraction-Based Neural Networks for Document Summarization",
        "date": "2016",
        "type": "abstractive",
        "dataset": ["CNN", "LCSTS"],
        "method": ["GRU", "Distraction", "Attention", "Beam"],
        "baseline": ["Luhn", "Edmundson", "LSA", "Lex-rank", "Sum-basic", "KL-sum"]
    },
    {
        "title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks",
        "date": "2016",
        "type": "abstractive",
        "dataset": ["Gigaword"],
        "method": ["CNN", "Attention", "RNN Decoder", "Position Embedding"],
        "baseline": ["ABS", "RAS-Elman", "RAS-LSTM"]
    },
    {
        "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["Arxiv", "PubMed"],
        "method": ["Hierarchical", "Discourse-Aware Decoder", "RNN", "Section RNN", "Section Attention", "Attention", "Copy", "Coverage", "Beam"],
        "baseline": ["Sum-basic", "Lex-rank", "LSA", "Attn-Seq2Seq", "Pointer"]
    },
    {
        "title": "A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Hierarchical", "Inconsistency Loss", "Attention", "Combined Attention", "GRU", "Pointer", "Coverage", "Multitask"],
        "baseline": ["DeepRL", "Pointer", "GAN"]
    },
    {
        "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
        "date": "2016",
        "type": "extractive",
        "dataset": ["CNN", "DUC"],
        "method": ["GRU", "Hierarchical", "Reranking", "Multitask", "word2vec"],
        "baseline": ["lead-3", "ILP", "URANK", "TGRAPH", "LReg"]
    },
    {
        "title": "Multi-Reward Reinforced Summarization with Saliency and Entailment",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN", "DUC"],
        "method": ["Multi-Reward", "Policy Gradient", "Entailment", "seq2seq", "LSTM", "Attention", "Coverage"],
        "baseline": ["SummmaRunner", "Pointer", "DeepRL"]
    },
    {
        "title": "Towards Improving Abstractive Summarization via Entailment Generation",
        "date": "2017",
        "type": "abstractive",
        "dataset": ["Gigaword", "SNLI"],
        "method": ["LSTM", "Bilinear Attention", "Entailment", "Multitask"],
        "baseline": ["ABS", "RAS-Elman"]
    },
    {
        "title": "A Deep Reinforced Model For Abstractive Summarization",
        "date": "2017",
        "type": "abstractive",
        "dataset": ["CNN", "NYT"],
        "method": ["LSTM", "Intra-Temporal Attention", "Intra-Decoder Attention", "Copy", "Weight Sharing", "Beam", "RL", "Policy Gradient", "Multitask"],
        "baseline": ["lead-3", "SummmaRunner", "Nallapati"]
    },
    {
        "title": "A Neural Attention Model for Sentence Summarization",
        "date": "2015",
        "type": "abstractive",
        "dataset": ["DUC-2004", "Gigaword"],
        "method": ["Bag-of-Word Encoder", "CNN", "Attention", "Beam", "Extractive Tuning"],
        "baseline": ["IR", "Prefix", "Compress", "WL", "Topiary", "Moses", "ABS"]
    },
    {
        "title": "Summary Level Training of Sentence Rewriting",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["BERT", "Sentence Rewriting", "Sentence Selection", "LSTM", "Copy", "RL", "Advantage Actor Critic"],
        "baseline": ["lead-3", "Refresh", "JECS", "HiBERT", "BERTSUM", "Pointer", "InconsisLoss", "Sentence Rewrite", "Bottom-Up", "Transformer"]
    },
    {
        "title": "Neural Summarization by Extracting Sentences and Words",
        "date": "2016",
        "type": "extractive",
        "dataset": ["DUC", "CNN"],
        "method": ["Copy", "CNN", "MaxPooling", "LSTM", "Attention", "Curriculum Learning", "Hierarchical", "Beam", "Entity Data"],
        "baseline": ["lead-3", "LReg", "ILP", "ABS", "TGRAPH", "URANK"]
    },
    {
        "title": "BANDITSUM: Extractive Summarization as Contextual Bandit",
        "date": "2018",
        "type": "extractive",
        "dataset": [],
        "method": ["Markov Decision Processes", "Contextual Bandit", "Policy Gradient", "LSTM", "MLP"],
        "baseline": ["lead-3", "SummmaRunner", "DQN", "Refresh", "RNES"]
    },
    {
        "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Transformer", "Multitask", "Language Model"],
        "baseline": ["lead-3", "Pointer", "Bottom-Up", "R3", "MASS"]
    },
    {
        "title": "Controllable Abstractive Summarization",
        "date": "2017",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["CNN", "GLU", "Attention", "Intra Attention", "BPE", "Length Constrained", "User Controlled", "Beam", "Trigram Decoding"],
        "baseline": ["lead-3", "Nallapati", "DeepRL", "fairseq"]
    },
    {
        "title": "Bottom-Up Abstractive Summarization",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["bottom-up Attention", "Pointer", "LSMT", "ELMo", "GLoVe", "Reranking"],
        "baseline": ["Pointer", "DeepRL", "Entailment", "InconsisLoss", "Sentence Rewrite"]
    },
    {
        "title": "Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN", "Gigaword"],
        "method": ["Multitask", "LSTM", "Pointer", "Coverage"],
        "baseline": ["Pointer", "ABS", "RAS-Elman", "lvt2k"]
    },
    {
        "title": "Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks",
        "date": "2018",
        "type": "extractive",
        "dataset": [],
        "method": ["Salient Sentence Selection", "Keyword Selection", "LSTM", "Pointer"],
        "baseline": ["lead-3", "SummmaRunner", "ABS"]
    },
    {
        "title": "Closed-Book Training to Improve Summarization Encoder Memory",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN", "DUC"],
        "method": ["Pointer", "RL", "Teacher", "RNN", "Attention"],
        "baseline": ["DeepRL", "Nallapati", "Pointer"]
    },
    {
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["XSum", "CNN"],
        "method": ["Denoising Autoencoder", "Transformer", "Multitask"],
        "baseline": ["lead-3", "Pointer", "UniLM", "BERTSUM"]
    },
    {
        "title": "Improving Neural Abstractive Document Summarization with Structural Regularization",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Hierarchical", "GRU", "Sentence By Sentence", "Word Decoder", "Sentence Decoder", "Sentence Attention", "Word Attention", "Structural-Compression", "Coverage", "Beam"],
        "baseline": ["SummmaRunner", "ABS", "graph attention", "Pointer"]
    },
    {
        "title": "Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Hierarchical", "GRU", "Sentence Selection", "Attention", "Distant Supervision"],
        "baseline": ["lead-3", "SummmaRunner", "ABS", "graph attention", "DeepRL", "Coverage"]
    },
    {
        "title": "An Editorial Network for Enhanced Document Summarization",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Hierarchical", "Pointer", "Soft Attention", "Encoder-Aligner-Decoder", "Teacher Forcing"],
        "baseline": ["lead-3", "SummmaRunner", "Refresh", "BanditSum", "Latent", "NeuSum", "Pointer", "KIGN", "Multi-Task", "DeepRL", "Entailment", "InconsisLoss", "Bottom-Up"]
    },
    {
        "title": "Neural Extractive Summarization with Side Information",
        "date": "2017",
        "type": "extractive",
        "dataset": ["CNN"],
        "method": ["CNN", "Hierarchical", "MaxPooling", "LSTM", "Beam", "word2vec"],
        "baseline": ["lead-3", "Pointer"]
    },
    {
        "title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
        "date": "2018",
        "type": "abstractive",
        "dataset": ["XSum"],
        "method": ["CNN", "GLU", "Attention", "Topic Distributions", "LDA", "Positional Embedding"],
        "baseline": ["random", "lead-3", "oracle", "seq2seq", "Pointer"]
    },
    {
        "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
        "date": "2018",
        "type": "extractive",
        "dataset": ["CNN"],
        "method": ["CNN", "LSTM", "MaxPooling", "RL", "Policy Gradient"],
        "baseline": ["lead-3", "Nallapati", "Cheng", "Refresh", "Pointer", "Tan"]
    },
    {
        "title": "ProphetNet: Prediction Future N-gram for Sequence-to-Sequence Pre-Training",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["CNN", "Gigaword"],
        "method": ["future n-gram prediction", "N-Stream Attention", "Transformer"],
        "baseline": ["lead-3", "Pointer", "S2S-Elmo", "Bottom-Up", "BERTSUM", "MASS", "UniLM"]
    },
    {
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "date": "2020",
        "type": "abstractive",
        "dataset": [],
        "method": ["Multitask", "Transformer"],
        "baseline": []
    },
    {
        "title": "Sequence Level Training with Recurrent Neural Networks",
        "date": "2015",
        "type": "abstractive",
        "dataset": ["Gigaword"],
        "method": ["LSTM", "RL"],
        "baseline": []
    },
    {
        "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["Gigaword"],
        "method": ["Transformer", "Sentence Reconstruction", "BPE"],
        "baseline": ["DAE", "BERT"]
    },
    {
        "title": "Controlling the Amount of Verbatim Copying in Abstractive Summarization",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["Gigaword", "Newsroom"],
        "method": ["Transformer", "BPE", "Reranking", "Beam"],
        "baseline": ["lvt5k", "Multi-Task", "SEASS", "DRGD", "Entailment", "Pointer", "R3", "BiSET", "Struct"]
    },
    {
        "title": "Abstractive Document Summarization with a Graph-Based Attentional Neural Model",
        "date": "2017",
        "type": "abstractive",
        "dataset": ["CNN"],
        "method": ["Graph-based Attention", "Hierarchical", "LSTM", "topic-sensitive PageRank"],
        "baseline": ["lead-3", "Luhn", "Edmundson", "LSA", "Lex-rank", "TextRank", "Sum-basic", "KL-sum", "Distraction", "LReg"]
    },
    {
        "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
        "date": "2019",
        "type": "abstractive",
        "dataset": [],
        "method": ["Template", "R3", "CNN", "Similarity", "Attention", "RNN"],
        "baseline": ["random", "retrieve", "ABS", "RAS-Elman", "Featseq2seq", "OpenNMT", "SEASS", "S2S-CGU", "FTSum", "R3"]
    },
    {
        "title": "Learning to Extract Coherent Summary via Deep Reinforcement Learning",
        "date": "2018",
        "type": "extractive",
        "dataset": ["CNN"],
        "method": ["RL", "Hierarchical", "CNN", "GRU", "MeanPooling", "Coherence"],
        "baseline": ["lead-3", "Nallapati", "Pointer", "NES"]
    },
    {
        "title": "Discourse-Aware Neural Extractive Text Summarization",
        "date": "2019",
        "type": "extractive",
        "dataset": ["CNN", "NYT"],
        "method": ["Structural discourse graphs", "GNN", "Transformer", "BERT"],
        "baseline": ["lead-3", "oracle", "NeuSum", "BanditSum", "JECS", "PointerBERT", "HiBERT", "BERTSUM", "T5"]
    },
    {
        "title": "Neural Latent Extractive Document Summarization",
        "date": "2018",
        "type": "extractive",
        "dataset": ["CNN"],
        "method": ["Latent Variable Model", "LSTM", "MeanPooling", "Encoder-Decoder"],
        "baseline": ["lead-3", "Pointer", "ABS", "SummmaRunner", "EXTRACT-CNN", "Refresh", "Extract", "Latent"]
    },
    {
        "title": "Abstract Text Summarization with a Convolutional Seq2Seq Model",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["Gigaword", "DUC"],
        "method": ["Copy", "Hierarchical", "CNN", "Sentence Selection", "Keyword Selection", "restricted Boltzmann machine", "RNN"],
        "baseline": ["ABS", "RAS-Elman", "GAN", "BERT"]
    },
    {
        "title": "HIBERT: Document Level Pre-training for Hierarchical Bidirectional Transformers for Document Summarization",
        "date": "2019",
        "type": "extractive",
        "dataset": ["CNN", "NYT"],
        "method": ["Transformer"],
        "baseline": ["Pointer", "DCA", "Sentence Rewrite", "InconsisLoss", "Bottom-Up", "SummmaRunner", "NeuSum", "Refresh", "BanditSum", "JECS", "Latent", "HierTransformer", "BERT"]
    },
    {
        "title": "Pretraining-Based Natural Language Generation for Text Summarization",
        "date": "2019",
        "type": "abstractive",
        "dataset": ["CNN", "NYT"],
        "method": ["BERT", "Transformer", "Drafting", "Copy"],
        "baseline": ["lead-3", "SummmaRunner", "Refresh", "DeepChannel", "MASK-LM", "RNN-Ext", "NeuSum",  "Pointer", "DeepRL", "InconsisLoss", "Bottom-Up", "DCA"]
    },
    {
        "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
        "date": "2020",
        "type": "abstractive",
        "dataset": ["XSum", "CNN", "Newsroom", "Multi-News", "Gigaword", "WikiHow", "Reddit TIFU", "BIGPATENT", "Arxiv", "PubMed", "AESLC", "BillSum"],
        "method": ["Transformer", "Sentence Generation", "Multitask"],
        "baseline": ["BERTShare", "MASS", "UniLM", "BART", "T5"]
    }
]